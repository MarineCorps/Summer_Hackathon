import pandas as pd
import traceback
import multiprocessing
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from .correlation_detect import detect_correlation, is_valid_column
import os


# âœ… ê°œë³„ ì»¬ëŸ¼ìŒ ë¶„ì„ í•¨ìˆ˜ (score + method ë°˜í™˜)
def analyze_columns(col1, col2):
    try:
        score, method = detect_correlation(col1, col2)
        return score, method
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {col1.name} vs {col2.name} â†’ {e}")
        return None, None


# âœ… íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì½ëŠ” ìœ í‹¸ í•¨ìˆ˜ (CSV + XLSX ìë™ ëŒ€ì‘ + ë‹¨ìœ„í–‰ ì œê±°)
def read_dataframe_safely(path):
    try:
        ext = os.path.splitext(path)[1].lower()
        if ext == ".csv":
            chunks = pd.read_csv(path, chunksize=10000)
            return pd.concat(chunks, ignore_index=True)

        elif ext == ".xlsx":
            # âœ… 1. í—¤ë” ìë™ íƒì§€
            preview = pd.read_excel(path, engine="openpyxl", header=None, nrows=15)
            for i in range(15):
                row = preview.iloc[i]
                text_row = row.astype(str).str.lower().fillna("")
                if text_row.str.contains("date|ë‚ ì§œ|ì‹œê°„|ì¼ì‹œ|ì¸¡ì •ì¼").any():
                    header_row = i
                    break
            else:
                header_row = 0

            # âœ… 2. ë‚ ì§œ ì»¬ëŸ¼ ë¯¸ë¦¬ ê°ì§€
            preview_named = pd.read_excel(path, engine="openpyxl", header=header_row, nrows=1)
            date_cols = [col for col in preview_named.columns if any(kw in str(col).lower() for kw in ["date", "time", "ë‚ ì§œ", "ì¼ì‹œ", "ì¸¡ì •ì¼"])]

            # âœ… 3. ìµœì¢… ì½ê¸°
            return pd.read_excel(path, engine="openpyxl", header=header_row, parse_dates=date_cols)

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")

    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {path} â†’ {e}")
        return pd.DataFrame()




# âœ… ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰í•  ë‹¨ì¼ ì‘ì—… ë‹¨ìœ„
def correlation_task(task_info):
    task_type = task_info['type']
    col1 = task_info['col1']
    col2 = task_info['col2']
    path1 = task_info['file1']
    path2 = task_info.get('file2')

    score, method = analyze_columns(col1, col2)
    if score is not None and abs(score) >= task_info['threshold']:
        if task_type == 'internal':
            return {
                "type": "internal",
                "file": path1,
                "col1": col1.name,
                "col2": col2.name,
                "score": score,
                "method": method
            }
        elif task_type == 'cross':
            return {
                "type": "cross",
                "file1": path1,
                "file2": path2,
                "col1": col1.name,
                "col2": col2.name,
                "score": score,
                "method": method
            }
    return None  # Noneì´ë©´ ê²°ê³¼ ë²„ë¦¼


# âœ… ë©”ì¸ ë¶„ì„ ì§„ì…ì  (ë‚´ë¶€ + êµì°¨ ì»¬ëŸ¼ â†’ ë³‘ë ¬ ì‹¤í–‰ + ì§„í–‰ë¥  í‘œì‹œ)
def analyze_all_columns(file_paths, threshold=0.3, max_workers=None):
    if max_workers is None:
        cpu_count = multiprocessing.cpu_count()
        max_workers = max(2, min(cpu_count, 8))  # ê³¼ë„í•œ ì“°ë ˆë“œ ë°©ì§€
        print(f"âš™ï¸ ì‚¬ìš© ê°€ëŠ¥ ë…¼ë¦¬ í”„ë¡œì„¸ì„œ: {cpu_count} â†’ ì„¤ì •ëœ ì‘ì—…ì ìˆ˜: {max_workers}")

    tasks = []

    # 1. ë‚´ë¶€ ì»¬ëŸ¼ ë¹„êµ (ìœ íš¨ ì»¬ëŸ¼ë§Œ í•„í„°ë§)
    for path in file_paths:
        df = read_dataframe_safely(path)
        if df.empty:
            continue

        valid_cols = [df[c] for c in df.columns if is_valid_column(df[c])]
        print(f"ğŸ” [ë‚´ë¶€ ë¹„êµ] {path} ìœ íš¨ ì»¬ëŸ¼ ìˆ˜: {len(valid_cols)}")

        for i in range(len(valid_cols)):
            for j in range(i + 1, len(valid_cols)):
                col1 = valid_cols[i]
                col2 = valid_cols[j]

                tasks.append({
                    "type": "internal",
                    "file1": path,
                    "col1": col1,
                    "col2": col2,
                    "threshold": threshold
                })

    # 2. ì„œë¡œ ë‹¤ë¥¸ íŒŒì¼ ê°„ì˜ ì»¬ëŸ¼ ë¹„êµ (ìœ íš¨ ì»¬ëŸ¼ë§Œ í•„í„°ë§)
    for i in range(len(file_paths)):
        for j in range(i + 1, len(file_paths)):
            path1 = file_paths[i]
            path2 = file_paths[j]

            df1 = read_dataframe_safely(path1)
            df2 = read_dataframe_safely(path2)
            if df1.empty or df2.empty:
                continue

            valid_cols1 = [df1[c] for c in df1.columns if is_valid_column(df1[c])]
            valid_cols2 = [df2[c] for c in df2.columns if is_valid_column(df2[c])]
            print(f"ğŸ” [í¬ë¡œìŠ¤ ë¹„êµ] {path1} Ã— {path2} â†’ ìœ íš¨ {len(valid_cols1)} x {len(valid_cols2)}")

            for col1 in valid_cols1:
                for col2 in valid_cols2:
                    tasks.append({
                        "type": "cross",
                        "file1": path1,
                        "file2": path2,
                        "col1": col1,
                        "col2": col2,
                        "threshold": threshold
                    })

    print(f"ğŸš€ ì´ ë¹„êµ ì‘ì—… ìˆ˜: {len(tasks)} â†’ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (max_workers={max_workers})")

    # 3. ë³‘ë ¬ ì²˜ë¦¬ + ì§„í–‰ë¥ 
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(correlation_task, task) for task in tasks]
        for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ” ë¶„ì„ ì§„í–‰ ì¤‘", ncols=90):
            try:
                result = future.result()
                if result:
                    yield result
            except Exception as e:
                print(f"âŒ ìŠ¤ë ˆë“œ ì˜ˆì™¸ ë°œìƒ: {e}")
                traceback.print_exc()
