from flask import Flask, request, jsonify, render_template
import pandas as pd
import os
import traceback
import json
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from utils.analysis_runner import analyze_csv_pair, analyze_internal_columns
from utils.gpt_analysis import analyze_with_gpt
from utils.batch_analyzer import analyze_all_columns
from utils.timeseries_detect import analyze_timeseries  # âœ… ì‹œê³„ì—´ ë¶„ì„ ëª¨ë“ˆ ì¶”ê°€

app = Flask(__name__, template_folder='templates')
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # ìµœëŒ€ ì—…ë¡œë“œ í¬ê¸° ì œí•œ (10MB)

UPLOAD_FOLDER = 'analyzer/uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# âœ… ì—…ë¡œë“œëœ íŒŒì¼ì„ Pandas DataFrameìœ¼ë¡œ ì½ê¸° (CSV / XLSX ì§€ì›)
def read_uploaded_file(path):
    ext = os.path.splitext(path)[1].lower()
    try:
        if ext == ".csv":
            return pd.read_csv(path, encoding='utf-8')

        elif ext == ".xlsx":
            header_row = auto_detect_excel_header(path)
            print(f"ğŸ“Œ [ì—‘ì…€] í—¤ë” ìë™ ê°ì§€ ê²°ê³¼: {header_row}í–‰")

            # 1ì°¨ ìŠ¤ìº”: ë‚ ì§œ ì»¬ëŸ¼ ì¶”ì •
            preview = pd.read_excel(path, engine="openpyxl", header=header_row, nrows=1)
            date_cols = [col for col in preview.columns if any(kw in str(col).lower() for kw in ["date", "time", "ë‚ ì§œ", "ì¼ì‹œ", "ì¸¡ì •ì¼"])]

            # ì‹¤ì œ ì½ê¸° (ë‚ ì§œ ì»¬ëŸ¼ ì§€ì •)
            return pd.read_excel(path, engine="openpyxl", header=header_row, parse_dates=date_cols)

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}")

    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {path} â†’ {e}")
        return pd.DataFrame()


# âœ… ìë™ í—¤ë” íƒì§€ í•¨ìˆ˜ (ì—‘ì…€ìš©)
def auto_detect_excel_header(path, max_rows=15):
    """
    ì—‘ì…€ íŒŒì¼ì—ì„œ 'ë‚ ì§œ', 'ì‹œê°„', 'ì¸¡ì •ì¼ì‹œ' ë“±ì˜ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í–‰ì„ í—¤ë”ë¡œ ìë™ ì¶”ì •
    """
    try:
        preview = pd.read_excel(path, engine="openpyxl", header=None, nrows=max_rows)

        for i in range(max_rows):
            row = preview.iloc[i]
            text_row = row.astype(str).str.lower().fillna("")
            if text_row.str.contains("date|ë‚ ì§œ|ì‹œê°„|ì¼ì‹œ|ì¸¡ì •ì¼").any():
                return i

    except Exception as e:
        print(f"âš ï¸ í—¤ë” ìë™ íƒì§€ ì‹¤íŒ¨: {e}")

    return 0  # ì‹¤íŒ¨í•˜ë©´ ì²« ë²ˆì§¸ í–‰ì„ ê¸°ë³¸ í—¤ë”ë¡œ ì‚¬ìš©

# âœ… ë©”ì¸ í˜ì´ì§€ ë¼ìš°íŒ…
@app.route('/')
def home():
    return render_template("index.html")


# âœ… 1. ë‹¨ì¼ ë¹„êµ ë¶„ì„ (file1 vs file2)
@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        file1 = request.files.get('file1')
        file2 = request.files.get('file2')
        if not file1 or not file2:
            return jsonify({"error": "file1ê³¼ file2 íŒŒì¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤."}), 400

        path1 = os.path.join(UPLOAD_FOLDER, file1.filename)
        path2 = os.path.join(UPLOAD_FOLDER, file2.filename)
        file1.save(path1)
        file2.save(path2)

        df1 = read_uploaded_file(path1)
        df2 = read_uploaded_file(path2)

        results = analyze_csv_pair(df1, df2) + analyze_internal_columns(df1) + analyze_internal_columns(df2)
        results.sort(key=lambda x: abs(x["score"]), reverse=True)

        response = {
            "results": results[:50],
            "matches": len(results)
        }

        print("âœ… [analyze] ìƒê´€ê´€ê³„ ê²°ê³¼ ìˆ˜:", len(results))
        enrich_response_with_visual_data(df1, response)

        if request.form.get("use_gpt", "false").lower() == "true":
            gpt_key = request.form.get("gpt_api_key", "")
            if gpt_key:
                df1_summary = df1.head(5).to_string()
                df2_summary = df2.head(5).to_string()
                response["gpt_summary"] = analyze_with_gpt(gpt_key, df1_summary, df2_summary)

        return jsonify(response)

    except Exception as e:
        print("âŒ [analyze] ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()
        return jsonify({"error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ", "detail": str(e)}), 500


# âœ… 2. ì „ì²´ ë¶„ì„ ì‹¤í–‰ (íŒŒì¼ ì—¬ëŸ¬ ê°œ â†’ ìƒê´€ê´€ê³„ or ì‹œê³„ì—´ ë¶„ì„)
@app.route('/analyze-all', methods=['POST'])
def analyze_all():
    try:
        files = request.files.getlist("files")
        paths = []

        for f in files:
            path = os.path.join(UPLOAD_FOLDER, f.filename)
            f.save(path)
            paths.append(path)

        if not paths:
            return jsonify({"error": "ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}), 400

        # âœ… ë¶„ì„ ìœ í˜• ë¶„ê¸° ì²˜ë¦¬
        analysis_type = request.form.get("analysis_type", "correlation")

        # âœ… [ì‹œê³„ì—´ ë¶„ì„] ë‹¤ì¤‘ íŒŒì¼ ëŒ€ì‘
        if analysis_type == "timeseries":
            results = {}

            for path in paths:
                filename = os.path.basename(path)
                df = read_uploaded_file(path)
                ts_result = analyze_timeseries(df)

                if not ts_result or "error" in ts_result:
                    results[filename] = {"error": ts_result.get("error", "ë¶„ì„ ì‹¤íŒ¨")}
                    continue

                results[filename] = ts_result

            return jsonify(results)

        # âœ… [ìƒê´€ê´€ê³„ ë¶„ì„] ê¸°ì¡´ ë°©ì‹ ìœ ì§€
        results = []
        result_path = os.path.join(UPLOAD_FOLDER, 'result.jsonl')
        with open(result_path, 'w', encoding='utf-8') as f_out:
            for r in analyze_all_columns(paths, threshold=0.3):
                f_out.write(json.dumps(r, ensure_ascii=False) + '\n')
                results.append(r)

        results.sort(key=lambda x: abs(x.get("score", 0)), reverse=True)
        df_first = read_uploaded_file(paths[0])

        response = {
            "results": results[:50],
            "matches": len(results),
            "result_file": result_path
        }

        enrich_response_with_visual_data(df_first, response)
        print("âœ… [analyze-all] ì‹œê°í™”ìš© í•„ë“œ í¬í•¨ ì—¬ë¶€:",
              [k for k in response.keys() if k.endswith('_data') or k == 'results'])

        return jsonify(response)

    except Exception as e:
        print("âŒ [analyze-all] ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

# âœ… 3. ì‹œê°í™”ìš© í•„ë“œ ìë™ ìƒì„± (line, box, bubble, radar, pie ë“±)
def enrich_response_with_visual_data(df, result: dict):
    try:
        if df.empty:
            print("âš ï¸ [enrich] ë¹„ì–´ ìˆëŠ” DataFrame, ì‹œê°í™” ìƒëµ")
            return

        print("ğŸ” [enrich] ì‹œê°í™” ì»¬ëŸ¼:", df.columns.tolist())
        num_cols = df.select_dtypes(include='number').columns[:3]

        # âœ… ì„ í˜• ì°¨íŠ¸ìš© (ì‹œê³„ì—´)
        if 'Date' in df.columns and len(num_cols) > 0:
            x_vals = df['Date'].astype(str).tolist()
            y_vals = df[num_cols[0]].dropna().tolist()
            if len(x_vals) == len(y_vals):
                result["line_data"] = {
                    "x": x_vals,
                    "y": y_vals,
                    "xLabel": "Date",
                    "yLabel": num_cols[0]
                }

        # âœ… ë°•ìŠ¤í”Œë¡¯ìš©
        if len(num_cols) >= 1:
            result["box_data"] = {
                "labels": list(num_cols),
                "dataList": [df[col].dropna().tolist() for col in num_cols]
            }

        # âœ… ë²„ë¸” / ë ˆì´ë”
        if len(num_cols) >= 3:
            raw_data = df[num_cols].dropna().to_numpy()
            scaler = MinMaxScaler()
            scaled = scaler.fit_transform(raw_data)
            result["bubble_data"] = {
                "x": scaled[:, 0].tolist(),
                "y": scaled[:, 1].tolist(),
                "size": (scaled[:, 2] * 100).tolist(),
                "xLabel": num_cols[0],
                "yLabel": num_cols[1]
            }

            radar_sample = scaled[:3]
            indicators = [{"name": col, "max": 1} for col in num_cols]
            result["radar_data"] = {
                "indicators": indicators,
                "valuesList": radar_sample.tolist(),
                "names": [f"Sample {i+1}" for i in range(len(radar_sample))]
            }

        # âœ… ë„ë„›ì°¨íŠ¸ìš© (ë²”ì£¼í˜•)
        cat_cols = df.select_dtypes(include='object').columns
        if len(cat_cols) > 0:
            top_cat = cat_cols[0]
            freq = df[top_cat].value_counts()
            result["pie_data"] = {
                "labels": freq.index.tolist(),
                "values": freq.values.tolist()
            }

    except Exception as e:
        print("âŒ [enrich] ì‹œê°í™” ë°ì´í„° ìƒì„± ì‹¤íŒ¨:", e)


# âœ… 4. ì‚°ì ë„ ë¶„ì„ìš© API (col1 vs col2)
@app.route('/scatter-data', methods=['GET'])
def scatter_data():
    try:
        col1 = request.args.get("col1")
        col2 = request.args.get("col2")
        if not col1 or not col2:
            return jsonify({"error": "col1, col2 íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400

        files = sorted(
            [f for f in os.listdir(UPLOAD_FOLDER) if f.endswith(('.csv', '.xlsx'))],
            key=lambda f: os.path.getmtime(os.path.join(UPLOAD_FOLDER, f)),
            reverse=True
        )

        if len(files) < 2:
            return jsonify({"error": "CSV ë˜ëŠ” XLSX íŒŒì¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."}), 400

        df1 = read_uploaded_file(os.path.join(UPLOAD_FOLDER, files[0]))
        df2 = read_uploaded_file(os.path.join(UPLOAD_FOLDER, files[1]))

        series1 = df1[col1] if col1 in df1 else df2.get(col1)
        series2 = df2[col2] if col2 in df2 else df1.get(col2)

        if series1 is None or series2 is None:
            return jsonify({"x": [], "y": []})

        if not pd.api.types.is_numeric_dtype(series1) or not pd.api.types.is_numeric_dtype(series2):
            return jsonify({"x": [], "y": []})

        series1 = series1.dropna().tolist()
        series2 = series2.dropna().tolist()
        length = min(len(series1), len(series2))

        return jsonify({
            "x": series1[:length],
            "y": series2[:length]
        })

    except Exception as e:
        print("âŒ [scatter-data] ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()
        return jsonify({"error": "ì„œë²„ ì˜¤ë¥˜", "detail": str(e)}), 500


# âœ… 5. ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸° (result.jsonl â†’ ì‹œê°í™”ìš©)
@app.route('/get-results', methods=['GET'])
def get_results():
    try:
        result_path = os.path.join(UPLOAD_FOLDER, 'result.jsonl')
        if not os.path.exists(result_path):
            return jsonify({"error": "ê²°ê³¼ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 404

        with open(result_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        results = [json.loads(line.strip()) for line in lines if line.strip()]
        results.sort(key=lambda x: abs(x.get("score", 0)), reverse=True)

        return jsonify({
            "results": results[:50],
            "matches": len(results)
        })

    except Exception as e:
        print("âŒ [get-results] ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()
        return jsonify({"error": "ì„œë²„ ì˜¤ë¥˜", "detail": str(e)}), 500


# âœ… ì„œë²„ ì‹¤í–‰
if __name__ == '__main__':
    app.run(port=5000, debug=True, use_reloader=False)
