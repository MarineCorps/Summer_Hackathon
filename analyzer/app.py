from flask import Flask, request, jsonify, render_template
import pandas as pd
import os
import traceback
import json
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from utils.analysis_runner import analyze_csv_pair, analyze_internal_columns
from utils.gpt_analysis import analyze_with_gpt
from utils.batch_analyzer import analyze_all_columns

app = Flask(__name__, template_folder='templates')
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # 10MB ì œí•œ

UPLOAD_FOLDER = 'analyzer/uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# âœ… ê³µí†µ íŒŒì¼ ë¡œë”© í•¨ìˆ˜ (.csv / .xlsx ì§€ì›)
def read_uploaded_file(path):
    ext = os.path.splitext(path)[1].lower()
    try:
        if ext == ".csv":
            return pd.read_csv(path, encoding='utf-8')
        elif ext == ".xlsx":
            return pd.read_excel(path, engine="openpyxl", header=5, skiprows=[6])
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {path} â†’ {e}")
        return pd.DataFrame()

@app.route('/')
def home():
    return render_template("index.html")


# âœ… 1. ë‹¨ì¼ ë¹„êµ ë¶„ì„
@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        file1 = request.files.get('file1')
        file2 = request.files.get('file2')
        if not file1 or not file2:
            return jsonify({"error": "file1ê³¼ file2 íŒŒì¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤."}), 400

        path1 = os.path.join(UPLOAD_FOLDER, file1.filename)
        path2 = os.path.join(UPLOAD_FOLDER, file2.filename)
        file1.save(path1)
        file2.save(path2)

        df1 = read_uploaded_file(path1)
        df2 = read_uploaded_file(path2)

        results = analyze_csv_pair(df1, df2) + analyze_internal_columns(df1) + analyze_internal_columns(df2)
        results.sort(key=lambda x: abs(x["score"]), reverse=True)

        response = {
            "results": results[:50],
            "matches": len(results)
        }

        print("âœ… [analyze] ìƒê´€ê´€ê³„ ê²°ê³¼ ìˆ˜:", len(results))
        enrich_response_with_visual_data(df1, response)

        if request.form.get("use_gpt", "false").lower() == "true":
            gpt_key = request.form.get("gpt_api_key", "")
            if gpt_key:
                df1_summary = df1.head(5).to_string()
                df2_summary = df2.head(5).to_string()
                response["gpt_summary"] = analyze_with_gpt(gpt_key, df1_summary, df2_summary)

        return jsonify(response)

    except Exception as e:
        print("âŒ [analyze] ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()
        return jsonify({"error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ", "detail": str(e)}), 500


# âœ… 2. ë‹¤ì¤‘ ë¶„ì„ ì‹¤í–‰ (ë©€í‹°ìŠ¤ë ˆë“œ + ì €ì¥)
@app.route('/analyze-all', methods=['POST'])
def analyze_all():
    try:
        files = request.files.getlist("files")
        paths = []

        for f in files:
            path = os.path.join(UPLOAD_FOLDER, f.filename)
            f.save(path)
            paths.append(path)

        if not paths:
            return jsonify({"error": "ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}), 400

        results = []
        result_path = os.path.join(UPLOAD_FOLDER, 'result.jsonl')
        with open(result_path, 'w', encoding='utf-8') as f_out:
            for r in analyze_all_columns(paths, threshold=0.3):
                f_out.write(json.dumps(r, ensure_ascii=False) + '\n')
                results.append(r)

        results.sort(key=lambda x: abs(x.get("score", 0)), reverse=True)

        print("âœ… [analyze-all] ë¶„ì„ ê²°ê³¼ ìˆ˜:", len(results))
        df_first = read_uploaded_file(paths[0])

        response = {
            "results": results[:50],
            "matches": len(results),
            "result_file": result_path
        }

        enrich_response_with_visual_data(df_first, response)
        print("âœ… [analyze-all] ì‹œê°í™”ìš© í•„ë“œ í¬í•¨ ì—¬ë¶€:",
              [k for k in response.keys() if k.endswith('_data') or k == 'results'])

        return jsonify(response)

    except Exception as e:
        print("âŒ [analyze-all] ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


# âœ… ì‹œê°í™”ìš© í•„ë“œ ì¶”ê°€
def enrich_response_with_visual_data(df, result: dict):
    try:
        if df.empty:
            print("âš ï¸ [enrich] ë¹„ì–´ ìˆëŠ” DataFrame, ì‹œê°í™” ìƒëµ")
            return

        print("ğŸ” [enrich] ì‹œê°í™” ì»¬ëŸ¼:", df.columns.tolist())
        num_cols = df.select_dtypes(include='number').columns[:3]

        if 'Date' in df.columns and len(num_cols) > 0:
            x_vals = df['Date'].astype(str).tolist()
            y_vals = df[num_cols[0]].dropna().tolist()
            if len(x_vals) == len(y_vals):
                result["line_data"] = {
                    "x": x_vals,
                    "y": y_vals,
                    "xLabel": "Date",
                    "yLabel": num_cols[0]
                }

        if len(num_cols) >= 1:
            result["box_data"] = {
                "labels": list(num_cols),
                "dataList": [df[col].dropna().tolist() for col in num_cols]
            }

        if len(num_cols) >= 3:
            raw_data = df[num_cols].dropna().to_numpy()
            scaler = MinMaxScaler()
            scaled = scaler.fit_transform(raw_data)
            result["bubble_data"] = {
                "x": scaled[:, 0].tolist(),
                "y": scaled[:, 1].tolist(),
                "size": (scaled[:, 2] * 100).tolist(),
                "xLabel": num_cols[0],
                "yLabel": num_cols[1]
            }

            radar_sample = scaled[:3]
            indicators = [{"name": col, "max": 1} for col in num_cols]
            result["radar_data"] = {
                "indicators": indicators,
                "valuesList": radar_sample.tolist(),
                "names": [f"Sample {i+1}" for i in range(len(radar_sample))]
            }

        cat_cols = df.select_dtypes(include='object').columns
        if len(cat_cols) > 0:
            top_cat = cat_cols[0]
            freq = df[top_cat].value_counts()
            result["pie_data"] = {
                "labels": freq.index.tolist(),
                "values": freq.values.tolist()
            }

    except Exception as e:
        print("âŒ [enrich] ì‹œê°í™” ë°ì´í„° ìƒì„± ì‹¤íŒ¨:", e)


# âœ… 4. ì‚°ì ë„ ìš”ì²­
@app.route('/scatter-data', methods=['GET'])
def scatter_data():
    try:
        col1 = request.args.get("col1")
        col2 = request.args.get("col2")
        if not col1 or not col2:
            return jsonify({"error": "col1, col2 íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400

        files = sorted(
            [f for f in os.listdir(UPLOAD_FOLDER) if f.endswith(('.csv', '.xlsx'))],
            key=lambda f: os.path.getmtime(os.path.join(UPLOAD_FOLDER, f)),
            reverse=True
        )

        if len(files) < 2:
            return jsonify({"error": "CSV ë˜ëŠ” XLSX íŒŒì¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."}), 400

        df1 = read_uploaded_file(os.path.join(UPLOAD_FOLDER, files[0]))
        df2 = read_uploaded_file(os.path.join(UPLOAD_FOLDER, files[1]))

        series1 = df1[col1] if col1 in df1 else df2.get(col1)
        series2 = df2[col2] if col2 in df2 else df1.get(col2)

        if series1 is None or series2 is None:
            return jsonify({"x": [], "y": []})

        if not pd.api.types.is_numeric_dtype(series1) or not pd.api.types.is_numeric_dtype(series2):
            return jsonify({"x": [], "y": []})

        series1 = series1.dropna().tolist()
        series2 = series2.dropna().tolist()
        length = min(len(series1), len(series2))

        return jsonify({
            "x": series1[:length],
            "y": series2[:length]
        })

    except Exception as e:
        print("âŒ [scatter-data] ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()
        return jsonify({"error": "ì„œë²„ ì˜¤ë¥˜", "detail": str(e)}), 500


# âœ… 5. ì €ì¥ëœ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
@app.route('/get-results', methods=['GET'])
def get_results():
    try:
        result_path = os.path.join(UPLOAD_FOLDER, 'result.jsonl')
        if not os.path.exists(result_path):
            return jsonify({"error": "ê²°ê³¼ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 404

        with open(result_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        results = [json.loads(line.strip()) for line in lines if line.strip()]
        results.sort(key=lambda x: abs(x.get("score", 0)), reverse=True)

        return jsonify({
            "results": results[:50],
            "matches": len(results)
        })

    except Exception as e:
        print("âŒ [get-results] ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()
        return jsonify({"error": "ì„œë²„ ì˜¤ë¥˜", "detail": str(e)}), 500


# âœ… ì„œë²„ ì‹¤í–‰
if __name__ == '__main__':
    app.run(port=5000, debug=True, use_reloader=False)
